\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\@writefile{toc}{\contentsline {chapter}{LIST OF TABLES}{xiii}{section*.1}}
\@writefile{toc}{\contentsline {chapter}{LIST OF FIGURES}{xiv}{section*.2}}
\citation{sparcc}
\citation{domen}
\@writefile{toc}{\contentsline {chapter}{LIST OF ABBREVIATIONS}{xxv}{section*.3}}
\citation{traud}
\citation{fortu2}
\citation{linkPredReview,collabComm}
\citation{larremoreparasite}
\citation{immuneClock}
\citation{Neuro}
\citation{socialnetwork}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Network Notation and Basic Summarization}{2}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Representing relational information}{2}{subsection.1.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces {\bf  A simple network example (coauthorship).} A co-authorship network with an edge between a pair of people if they have written a paper together.}}{2}{figure.1.1}}
\newlabel{fig:social}{{1.1}{2}{{\bf A simple network example (coauthorship).} A co-authorship network with an edge between a pair of people if they have written a paper together}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Network Summary Statistics}{3}{subsection.1.1.2}}
\citation{wong2015}
\citation{cytofkit}
\citation{cytof}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces {\bf  Hairball network.} Networks are often noisy data structures and lack an immediate straight forward structural interpretation. \emph  {Image from \url  {https://cs.umd.edu}.}}}{4}{figure.1.2}}
\newlabel{fig:Hairball}{{1.2}{4}{{\bf Hairball network.} Networks are often noisy data structures and lack an immediate straight forward structural interpretation. \emph {Image from \url {https://cs.umd.edu}.}\relax }{figure.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2.1}Example: A network representation of single cell data and simple summary statistics}{4}{subsubsection.1.1.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2.2}Degree Distribution}{4}{subsubsection.1.1.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces {\bf  Network of single cells.} We constructed a network from mass cytometry profiling among 500 cells in single cell dataset. Each cell has 52 measured immune features. In this network, each node is a single cell and is connected to its 5 nearest neighbors.}}{5}{figure.1.3}}
\newlabel{fig:Cytometry}{{1.3}{5}{{\bf Network of single cells.} We constructed a network from mass cytometry profiling among 500 cells in single cell dataset. Each cell has 52 measured immune features. In this network, each node is a single cell and is connected to its 5 nearest neighbors}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces {\bf  Degree distribution for the single cell network.} We visualize the degree distribution in the single cell network presented in Figure \ref  {fig:Cytometry}. {\bf  A.} We compute a cumulative distribution plot for degree. {\bf  B.} Node degrees can also be visualized with a simple histogram.}}{6}{figure.1.4}}
\newlabel{fig:DegDist}{{1.4}{6}{{\bf Degree distribution for the single cell network.} We visualize the degree distribution in the single cell network presented in Figure \ref {fig:Cytometry}. {\bf A.} We compute a cumulative distribution plot for degree. {\bf B.} Node degrees can also be visualized with a simple histogram}{figure.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2.3}Centrality}{6}{subsubsection.1.1.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces {\bf  Centralities on the single cell network.} The second order ego network for the highest centrality nodes in the single cell network according to degree, betweenness, and eigenvector in the left, center, and right plots, respectively. These plots are meant to emphasize how each of these centrality measures prioritizes different kind of stucture.}}{7}{figure.1.5}}
\newlabel{fig:Centrality}{{1.5}{7}{{\bf Centralities on the single cell network.} The second order ego network for the highest centrality nodes in the single cell network according to degree, betweenness, and eigenvector in the left, center, and right plots, respectively. These plots are meant to emphasize how each of these centrality measures prioritizes different kind of stucture}{figure.1.5}{}}
\citation{betzel}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Introduction to community detection}{8}{section.1.2}}
\citation{fortu1,fortu2,shaicase,muchacommunity}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces {\bf  Assortative Community Structure.} This network is an example of assortative community structure, where nodes are tightly connected to each other and more sparsely connected to the rest of the network. Each community is outlined with a pink dotted line.}}{9}{figure.1.6}}
\newlabel{fig:Assort}{{1.6}{9}{{\bf Assortative Community Structure.} This network is an example of assortative community structure, where nodes are tightly connected to each other and more sparsely connected to the rest of the network. Each community is outlined with a pink dotted line}{figure.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Community detection methods}{9}{section.1.3}}
\citation{newman2006modularity}
\citation{benderCanfield}
\citation{newmangirvan}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Notation for Community Detection}{10}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Quality function maximization with modularity}{10}{subsection.1.3.2}}
\citation{resParam}
\citation{blondel}
\citation{hierarchicalmod}
\citation{browet}
\citation{TwoD}
\citation{originalSBM}
\citation{affil}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Identifying communities with probabilistic approaches}{11}{subsection.1.3.3}}
\citation{koller}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces {\bf  A comparison of $k$-means and the Louvain algorithm on the single cell network.} A comparison of the results of clustering results on the the single cell dataset through $k$-means on the original 52-dimensional data (left) and by the Louvain algorithm on the nearest neighbor network (right). Each of the single cells (or nodes in the nearest neighbor network) is visualized by a 2-dimensional projection frin tSNE. Points are colored by their cluster membership under $k$-means on the original data (left) and Louvain community detection (right). Applying community detection to the nearest neighbor network seems to smooth out the partition and identify some smaller clusters. }}{12}{figure.1.7}}
\newlabel{fig:clustering}{{1.7}{12}{{\bf A comparison of $k$-means and the Louvain algorithm on the single cell network.} A comparison of the results of clustering results on the the single cell dataset through $k$-means on the original 52-dimensional data (left) and by the Louvain algorithm on the nearest neighbor network (right). Each of the single cells (or nodes in the nearest neighbor network) is visualized by a 2-dimensional projection frin tSNE. Points are colored by their cluster membership under $k$-means on the original data (left) and Louvain community detection (right). Applying community detection to the nearest neighbor network seems to smooth out the partition and identify some smaller clusters}{figure.1.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3.1}Probabilistic graphical models for statistical inference}{12}{subsubsection.1.3.3.1}}
\newlabel{pgm}{{1.3.3.1}{12}{Probabilistic graphical models for statistical inference\relax }{subsubsection.1.3.3.1}{}}
\citation{koller}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces {\bf  Directed Acyclic Graph.} A directed acyclic graph (DAG) is formed based on dependency between random variable and allows for a fully factorized probability distribution. Nodes represent random variables and a directed edge from node $i$ to node $j$ indicates that node $j$ depends on node $i$.}}{13}{figure.1.8}}
\newlabel{fig:DAG}{{1.8}{13}{{\bf Directed Acyclic Graph.} A directed acyclic graph (DAG) is formed based on dependency between random variable and allows for a fully factorized probability distribution. Nodes represent random variables and a directed edge from node $i$ to node $j$ indicates that node $j$ depends on node $i$}{figure.1.8}{}}
\citation{sbmorig}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces {\bf  SBM Graphical Model.} A graphical model is used to model the dependency between the node-to-community assignments, ${\bf  z}$ and the observed network adjacency matrix, ${\bf  A}$.}}{14}{figure.1.9}}
\newlabel{fig:graphical}{{1.9}{14}{{\bf SBM Graphical Model.} A graphical model is used to model the dependency between the node-to-community assignments, ${\bf z}$ and the observed network adjacency matrix, ${\bf A}$}{figure.1.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3.2}Stochastic Block Model}{14}{subsubsection.1.3.3.2}}
\citation{dudin}
\citation{jakk}
\citation{dudin}
\citation{dudin}
\citation{comp}
\citation{belief}
\citation{aicher,peix}
\citation{vBayes}
\citation{peix}
\citation{degreecorrectSBM}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3.3}Variants to the Classic Stochastic Block Model}{17}{subsubsection.1.3.3.3}}
\citation{mixMember}
\citation{LA}
\citation{mixMember}
\citation{LA}
\citation{mixMember}
\citation{pluralHom}
\citation{bigclam}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3.4}Affiliation model and inference}{19}{subsubsection.1.3.3.4}}
\newlabel{eqAff}{{1.17}{19}{Affiliation model and inference\relax }{equation.1.3.17}{}}
\citation{deepWalk}
\citation{kMean}
\citation{rWalk,gleichpagerank}
\citation{word2Vec}
\citation{node2vec}
\citation{homophily}
\citation{structural}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Deep Learning Approaches}{20}{subsection.1.3.4}}
\citation{Benson}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Higher order network analysis}{21}{subsection.1.3.5}}
\citation{laplacian}
\citation{benson2}
\citation{systemsImmuno,deepGenome,machineGenomics}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Community detection in computational biology}{22}{section.1.4}}
\citation{immuneClock}
\citation{cytof}
\citation{eNet}
\citation{gut}
\citation{networkMicrobiome}
\citation{moduleMicrobiome}
\citation{girvancommunity}
\citation{Rand}
\citation{hub}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Immunological profiling to establish a pregnancy immune clock}{23}{subsection.1.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Uncovering differences in microbiome community structure in patients with inflammatory bowel disease}{24}{subsection.1.4.2}}
\citation{flow}
\citation{nimaFlow}
\citation{FlowSpectral}
\citation{spectral1}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Community detection for analysis of flow cytometry data}{25}{subsection.1.4.3}}
\citation{varIntro}
\citation{larremoreparasite}
\citation{degreeCorrect}
\citation{VI}
\citation{newmanAssort}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Understanding genetic diversity of the malaria parasite genes}{26}{subsection.1.4.4}}
\citation{larremoreparasite}
\citation{phenoGraph}
\citation{intraTumor}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Analysis of high dimensional single cell data for tumor heterogeneity}{28}{subsection.1.4.5}}
\citation{RWR}
\citation{parker2015network}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}Identification of virulence factor genes related to antibiotic resistance of uropathogenic \emph  {E. coli}}{29}{subsection.1.4.6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Thesis Contribution}{30}{section.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Thesis Statement}{30}{subsection.1.5.1}}
\citation{catala}
\citation{mlsbm1}
\citation{airoldi}
\citation{peng}
\citation{SuperNodeSide}
\citation{gilbert}
\citation{clauset}
\citation{ilouvain}
\citation{cesna}
\citation{peel2017ground}
\citation{peel2017ground}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Thesis organization}{31}{subsection.1.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}Summary of the novelty of this work}{32}{subsection.1.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces {\bf  Summarizing the novelty of our 3 developed methods}. For each of the 3 methods we developed, we provide a brief description of what it does, the top 3 most similar approaches, and why our approach is novel.}}{32}{table.1.1}}
\newlabel{Tab11}{{1.1}{32}{{\bf Summarizing the novelty of our 3 developed methods}. For each of the 3 methods we developed, we provide a brief description of what it does, the top 3 most similar approaches, and why our approach is novel}{table.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.4}Relevant Publications}{32}{subsection.1.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.5}Software}{33}{subsection.1.5.5}}
\citation{ohmNet}
\citation{microbiome}
\citation{kivelamultilayer,boccaletti2014structure,manlioMathFoundations}
\citation{genetic}
\citation{socialnetwork}
\citation{muchamultislice}
\citation{manlioMathFoundations}
\citation{muchamultislice}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Strata Multilayer Stochastic Block Model}{34}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{domen}
\citation{biclustering}
\citation{cocluster}
\citation{cocluster}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{35}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Introduction to multilayer networks}{35}{subsection.2.1.1}}
\citation{ugander2013subgraph,motiffinding}
\citation{taxonomy,NONCluster,confusingMesoscopic}
\citation{structurenetwork}
\citation{netensemble}
\citation{porter2009communities,fortunato}
\citation{rombach2014core}
\citation{newmanmodularity}
\citation{abby}
\citation{community,fortunato,leskoveccommunity,clausethierarchy,newmanspectral}
\citation{NONCluster}
\citation{confusingMesoscopic}
\citation{originalSBM}
\citation{rombach2014core,aicher}
\citation{abby,guimera2009missing}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Comparing network layers based on community structure}{36}{subsection.2.1.2}}
\citation{muchamultislice}
\citation{manlio2}
\citation{airoldi,mlsbm1,barbillon,catala,thiagomlsbm}
\citation{airoldi,mlsbm1,barbillon}
\citation{airoldi}
\citation{dudin}
\citation{airoldi}
\citation{mlsbm1}
\citation{barbillon}
\citation{airoldi}
\citation{mlsbm1}
\citation{catala}
\citation{thiagomlsbm}
\citation{catala}
\citation{thiagomlsbm}
\citation{domen}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Related work in community detection of multilayer networks}{38}{subsection.2.1.3}}
\citation{domen}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}A Summary of Novel Contributions of sMLSBM}{40}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Methods}{40}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}sMLSBM Model Definition}{40}{subsection.2.2.1}}
\newlabel{goals}{{2.1.4}{41}{A Summary of Novel Contributions of sMLSBM\relax }{subsection.2.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces {\bf  Objective of strata multilayer stochastic block model (sMLSBM)}. Each of the $L=9$ networks here represents a layer in a multilayer network. Every network layer has $N=36$ nodes that are consistent across all layers. There are $S=3$ strata as indicated by the three rows and the colors of nodes. Clearly, network layers within a stratum exhibit strong similarities in community structure. That is, although each layer follows an SBM with $K=3$ communities, the SBM parameters are identical for layers within a strata but differ between layers in different strata. We would like to partition the layers into their appropriate strata and learn their associated SBM parameters, $\pi ^s$ and $Z^s$.}}{41}{figure.2.1}}
\newlabel{M1}{{2.1}{41}{{\bf Objective of strata multilayer stochastic block model (sMLSBM)}. Each of the $L=9$ networks here represents a layer in a multilayer network. Every network layer has $N=36$ nodes that are consistent across all layers. There are $S=3$ strata as indicated by the three rows and the colors of nodes. Clearly, network layers within a stratum exhibit strong similarities in community structure. That is, although each layer follows an SBM with $K=3$ communities, the SBM parameters are identical for layers within a strata but differ between layers in different strata. We would like to partition the layers into their appropriate strata and learn their associated SBM parameters, $\pi ^s$ and $Z^s$}{figure.2.1}{}}
\citation{dudin}
\citation{gap}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Inference for learning model parameters of sMLSBM}{42}{subsection.2.2.2}}
\newlabel{eq1}{{2.1}{42}{Inference for learning model parameters of sMLSBM\relax }{equation.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces {\bf  Schematic illustration of our algorithm}: Our algorithm for fitting an sMLSBM is broken up into two phases: an initialization phase to cluster layers into strata, and an iterative phase that allows learning of node-to-community and layer-to-strata assignments.}}{43}{figure.2.2}}
\newlabel{fig:Schematic}{{2.2}{43}{{\bf Schematic illustration of our algorithm}: Our algorithm for fitting an sMLSBM is broken up into two phases: an initialization phase to cluster layers into strata, and an iterative phase that allows learning of node-to-community and layer-to-strata assignments}{figure.2.2}{}}
\citation{dudin}
\citation{airoldi}
\citation{dempster}
\citation{dudin}
\citation{airoldi,barbillon,dudin}
\citation{Dudin}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Results}{47}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Synthetic Examples}{47}{subsection.2.3.1}}
\citation{commdeccompare}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.1}Comparison of sMLSBM to other SBM Approaches}{48}{subsubsection.2.3.1.1}}
\newlabel{sec:SBM1}{{2.3.1.1}{48}{Comparison of sMLSBM to other SBM Approaches\relax }{subsubsection.2.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces {\bf  \noindent Synthetic experiment comparing sMLSBM to other SBMs.} {\bf  A}.\nobreakspace  {}We specified a model with $S=3$ strata and $L=10$ layers per stratum. A representative layer from each stratum is plotted. Note that nodes in all networks are colored according to their community membership in stratum 1. Each network has $N=128$ nodes, $K=4$ communities and mean degree, $c=20$. The $p_{in}^s$ parameters for $s=1,$ $2$ and 3 are 0.6, 0.4 and 0.25, respectively. Corresponding values of $p_{out}^s$ were selected to maintain the desired expected mean degree, c=20. {\bf  B}. We fit 3 types of models to the 30 network layers: i) single SBM: fitting a single SBM to all of the layers; ii) single-Layer SBM: fitting an individual SBM to each layer; and iii) sMLSBM: identifying strata and fitting an SBMs for each strata. Each model yields an estimate $\overline  {{\boldsymbol  \pi }^{s_l}}$ for the true SBM of each layer $l$, which is denoted ${{\boldsymbol  \pi }}^{l}$. Here $s_l$ denotes the inferred strata for layer $l$. On the vertical axis we plot the mean $\ell $2 norm error $||\text  {vec}({\boldsymbol  \pi ^{l})}-\text  {vec}(\overline  {{\boldsymbol  \pi }^{s_{l}}})||_{2}$. {\bf  C}. For each of the three models, we computed the normalized mutual information (NMI) between the true node-to-community assignments ${{\bf  z}^{l}}$ and the inferred values $\overline  {{\bf  z}^{s_l}}$.}}{49}{figure.2.3}}
\newlabel{fig:SynExp1}{{2.3}{49}{{\bf \noindent Synthetic experiment comparing sMLSBM to other SBMs.} {\bf A}.~We specified a model with $S=3$ strata and $L=10$ layers per stratum. A representative layer from each stratum is plotted. Note that nodes in all networks are colored according to their community membership in stratum 1. Each network has $N=128$ nodes, $K=4$ communities and mean degree, $c=20$. The $p_{in}^s$ parameters for $s=1,$ $2$ and 3 are 0.6, 0.4 and 0.25, respectively. Corresponding values of $p_{out}^s$ were selected to maintain the desired expected mean degree, c=20. {\bf B}. We fit 3 types of models to the 30 network layers: i) single SBM: fitting a single SBM to all of the layers; ii) single-Layer SBM: fitting an individual SBM to each layer; and iii) sMLSBM: identifying strata and fitting an SBMs for each strata. Each model yields an estimate $\overline {{\boldsymbol \pi }^{s_l}}$ for the true SBM of each layer $l$, which is denoted ${{\boldsymbol \pi }}^{l}$. Here $s_l$ denotes the inferred strata for layer $l$. On the vertical axis we plot the mean $\ell $2 norm error $||\text {vec}({\boldsymbol \pi ^{l})}-\text {vec}(\overline {{\boldsymbol \pi }^{s_{l}}})||_{2}$. {\bf C}. For each of the three models, we computed the normalized mutual information (NMI) between the true node-to-community assignments ${{\bf z}^{l}}$ and the inferred values $\overline {{\bf z}^{s_l}}$}{figure.2.3}{}}
\citation{decelle2011inference}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.2}Synthetic Experiment with Two Strata}{50}{subsubsection.2.3.1.2}}
\newlabel{sec:2strata}{{2.3.1.2}{50}{Synthetic Experiment with Two Strata\relax }{subsubsection.2.3.1.2}{}}
\citation{microbiome}
\citation{microbeco}
\citation{sparcc}
\citation{sparcc}
\citation{sparcc}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Human Microbiome Project Example}{51}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  {\bf  Synthetic experiment with two strata.} We conducted numerical experiments with multilayer networks with $N=128$ nodes, mean degree $c=16$, $S=2$ strata and $K^1=K^2=4$ communities. The networks contained either $L=10$ (left column) or $L=100$ layers (right column), which were divided equally into the two strata. For stratum 1, we fixed the quantity $N(p_{{in}}^{1}-p_{{out}}^{1})=10$, which fully specifies $(p_{{in}}^{1},p_{{out}}^{1})$ since setting $c=16$ also constrains these parameters. In contrast, we vary $N(p_{{in}}^{2}-p_{{out}}^{2})$. {\bf  A}. As a function of $N(p_{{in}}^{2}-p_{{out}}^{2})$, we plot the mean NMI to interpret the ability of sMLSBM to recover the true layer-to-strata assignments. We compare the performance of sMLSBM (purple curve) to generic $k$-means clustering (green symbols) of adjacency matrices. {\bf  B.} We plot the mean number of iterations (NOI) required for Phase II of our algorithm to converge. {\bf  C.} Finally, we measure the quality of node-to-community assignment results by plotting the mean NMI between the true node-to-community assignments and those inferred with sMLSBM in stratum 1 (red symbols) and stratum 2 (blue symbols).}}{52}{figure.2.4}}
\newlabel{fig:saray}{{2.4}{52}{{\bf Synthetic experiment with two strata.} We conducted numerical experiments with multilayer networks with $N=128$ nodes, mean degree $c=16$, $S=2$ strata and $K^1=K^2=4$ communities. The networks contained either $L=10$ (left column) or $L=100$ layers (right column), which were divided equally into the two strata. For stratum 1, we fixed the quantity $N(p_{{in}}^{1}-p_{{out}}^{1})=10$, which fully specifies $(p_{{in}}^{1},p_{{out}}^{1})$ since setting $c=16$ also constrains these parameters. In contrast, we vary $N(p_{{in}}^{2}-p_{{out}}^{2})$. {\bf A}. As a function of $N(p_{{in}}^{2}-p_{{out}}^{2})$, we plot the mean NMI to interpret the ability of sMLSBM to recover the true layer-to-strata assignments. We compare the performance of sMLSBM (purple curve) to generic $k$-means clustering (green symbols) of adjacency matrices. {\bf B.} We plot the mean number of iterations (NOI) required for Phase II of our algorithm to converge. {\bf C.} Finally, we measure the quality of node-to-community assignment results by plotting the mean NMI between the true node-to-community assignments and those inferred with sMLSBM in stratum 1 (red symbols) and stratum 2 (blue symbols)}{figure.2.4}{}}
\citation{domen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.1}Comparison of sMLSBM to multilayer network reducibility}{53}{subsubsection.2.3.2.1}}
\citation{dingcluster}
\citation{dingcluster}
\citation{sparcc}
\citation{domen}
\citation{sparcc}
\citation{domen}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2.2}Generating samples from the fitted sMLSBM}{54}{subsubsection.2.3.2.2}}
\citation{domen}
\citation{taylor2015enhanced}
\citation{domen}
\citation{domen}
\citation{weightSBM}
\citation{sbmdirect}
\citation{degreecorrectSBM}
\citation{antagonism}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces {\bf  Comparison of sMLSBM on the OTU interaction networks \citep  {sparcc} for each of the body sites to a reducibility hierarchy \citep  {domen}.} As described in the text, we consider a multiplex network with $L=18$ layers and $N=213$ nodes, which we group here into $S=6$ strata, while the dendrogram was generated by the method employed as the precursor to the reducibility framework. Colored boxes around the leaves of the dendrogram designate the body site to strata assignments obtained with sMLSBM.}}{55}{figure.2.5}}
\newlabel{M5}{{2.5}{55}{{\bf Comparison of sMLSBM on the OTU interaction networks \cite {sparcc} for each of the body sites to a reducibility hierarchy \cite {domen}.} As described in the text, we consider a multiplex network with $L=18$ layers and $N=213$ nodes, which we group here into $S=6$ strata, while the dendrogram was generated by the method employed as the precursor to the reducibility framework. Colored boxes around the leaves of the dendrogram designate the body site to strata assignments obtained with sMLSBM}{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces {\bf  Visualization of Strata in SparCC Networks.} We visualize the adjacency matrices for SparCC networks that encode microbiome interactions at body sites. In each panel, a colored dot at position $(i,j)$ indicates the existence of an edge $(i,j)$ in the corresponding network layer. The four rows correspond to four different strata. In column 1, we show a sample network generated from the SBM parameters, $\overline  {{\boldsymbol  \pi }^{s}}$ and $\overline  {{\bf  Z}^{s}}$, that we inferred for that stratum. In Columns 2 and 3, we show SparCC networks from that particular stratum. Note the strong similarity across each row.}}{56}{figure.2.6}}
\newlabel{sampleNet}{{2.6}{56}{{\bf Visualization of Strata in SparCC Networks.} We visualize the adjacency matrices for SparCC networks that encode microbiome interactions at body sites. In each panel, a colored dot at position $(i,j)$ indicates the existence of an edge $(i,j)$ in the corresponding network layer. The four rows correspond to four different strata. In column 1, we show a sample network generated from the SBM parameters, $\overline {{\boldsymbol \pi }^{s}}$ and $\overline {{\bf Z}^{s}}$, that we inferred for that stratum. In Columns 2 and 3, we show SparCC networks from that particular stratum. Note the strong similarity across each row}{figure.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Conclusion}{57}{section.2.4}}
\citation{taylor2015enhanced}
\citation{detect20,detect21,detect22,detect23,detect24,detect25}
\citation{detectDegreeHetero}
\citation{peixotoHierarchAttribute,HierarchAttl}
\citation{detectTemporal}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Detectability in a single stratum}{58}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Introduction to detectability}{58}{subsection.2.5.1}}
\citation{detect23,detect24}
\citation{detect23}
\citation{detect24}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Studying detectability in two block networks}{59}{subsection.2.5.2}}
\newlabel{detectEquation}{{2.14}{59}{Studying detectability in two block networks\relax }{equation.2.5.14}{}}
\citation{detect38,detect39}
\citation{detect24,peixotoHierarchAttribute,HierarchAttl}
\citation{newmangirvan}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Using random matrix theory to study detectability}{60}{subsection.2.5.3}}
\citation{taylor2015enhanced}
\citation{taylor2015enhanced}
\citation{airoldi}
\citation{detect25}
\citation{smlsbm}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}Results}{62}{subsection.2.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Conclusion}{62}{subsection.2.5.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces {\bf  Layer aggregation enhances the detectability of community structure.} Layer aggregation enhances the detectability of community structure. (a),(b). We plot the detectability limit $\Delta ^{*}$ versus mean edge probability $\rho $ for a single network layer (red dot-dashed curves), the aggregate network obtained by summation (blue dashed curves), and aggregate networks obtained by thresholding this summation at $\mathaccentV {tilde}07E{L} \in \{1,2,3,4\}$ (solid curves). Gold circles and cyan squares highlight $\mathaccentV {tilde}07E{L}=L$ and $\mathaccentV {tilde}07E{L}=1$, which we refer to as AND and OR networks, respectively. Results are shown for $N=10^{4}$ nodes with (a) $L=4$ and (b) $L=16$ layers. (c) For $L=4$, we show $\Delta ^{*}$ versus $\rho $ for the optimal threshold $\mathaccentV {tilde}07E{L}=\ceil  *{\rho L}$ (orange triangles), which lies on the solution curves for $\mathaccentV {tilde}07E{L} \in \{1,\dots  ,L\}$ (solid curves). (d) We show $\Delta ^{*}$ for $\mathaccentV {tilde}07E{L}=\ceil  *{\rho L}$ with $L \in \{4,16\}$. These piecewise-continuous solutions collapse onto the asymptotic solution $\delta _{\text  {asym}}^{*}$ (black curve) as $L$ increases. In panels (c), (d), we additionally plot $\delta ^{*}$ for the summation network (blue dashed curves). }}{63}{figure.2.7}}
\newlabel{Detect}{{2.7}{63}{{\bf Layer aggregation enhances the detectability of community structure.} Layer aggregation enhances the detectability of community structure. (a),(b). We plot the detectability limit $\Delta ^{*}$ versus mean edge probability $\rho $ for a single network layer (red dot-dashed curves), the aggregate network obtained by summation (blue dashed curves), and aggregate networks obtained by thresholding this summation at $\tilde {L} \in \{1,2,3,4\}$ (solid curves). Gold circles and cyan squares highlight $\tilde {L}=L$ and $\tilde {L}=1$, which we refer to as AND and OR networks, respectively. Results are shown for $N=10^{4}$ nodes with (a) $L=4$ and (b) $L=16$ layers. (c) For $L=4$, we show $\Delta ^{*}$ versus $\rho $ for the optimal threshold $\tilde {L}=\ceil *{\rho L}$ (orange triangles), which lies on the solution curves for $\tilde {L} \in \{1,\dots ,L\}$ (solid curves). (d) We show $\Delta ^{*}$ for $\tilde {L}=\ceil *{\rho L}$ with $L \in \{4,16\}$. These piecewise-continuous solutions collapse onto the asymptotic solution $\delta _{\text {asym}}^{*}$ (black curve) as $L$ increases. In panels (c), (d), we additionally plot $\delta ^{*}$ for the summation network (blue dashed curves)}{figure.2.7}{}}
\citation{compressing}
\citation{browet}
\citation{slic}
\citation{slic}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Network compression for community detection with super nodes}{65}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Representing images with super pixels before segmentation}{65}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces {\bf  Superpixel coarse-graining of an image before segmentation.} An image can be represented by a $1147 \times 1147$ grid of pixels (left). Representing the image with 600 super pixels (right), reduces the size of the image and hence the segmentation problem is to partition the set of 600 super pixels. }}{66}{figure.3.1}}
\newlabel{fig:superPix}{{3.1}{66}{{\bf Superpixel coarse-graining of an image before segmentation.} An image can be represented by a $1147 \times 1147$ grid of pixels (left). Representing the image with 600 super pixels (right), reduces the size of the image and hence the segmentation problem is to partition the set of 600 super pixels}{figure.3.1}{}}
\citation{danon}
\citation{jureTruth}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Introduction}{67}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Super node pre-processing for networks}{67}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.1}Problem Formulation}{67}{subsubsection.3.2.1.1}}
\newlabel{UnderSeg}{{3.1}{67}{Problem Formulation\relax }{equation.3.2.1}{}}
\newlabel{NMI}{{3.2}{67}{Problem Formulation\relax }{equation.3.2.2}{}}
\citation{supergenomic,SuperNodeSide,gilbert,peng}
\citation{SuperNodeSide}
\citation{peng}
\citation{SuperNodeSide}
\citation{supergenomic}
\citation{supergenomic}
\citation{SuperNodeSide}
\citation{gilbert}
\citation{peng}
\citation{gilbert}
\citation{peng}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.2}An opportunity for super nodes in community detection}{68}{subsubsection.3.2.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Related Work}{68}{subsection.3.2.2}}
\citation{blondel}
\citation{tiagosbm}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Validation metrics for a quality super node representation}{70}{subsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces {\bf  Defining super nodes}. To define the super node representation of a network, we select $S$ seeds and agglomerate local regions around them to create super nodes. This then leads to a new network with weighted edges between the $S$ super nodes upon which community detection can be more efficiently applied.}}{71}{figure.3.2}}
\newlabel{Fig1}{{3.2}{71}{{\bf Defining super nodes}. To define the super node representation of a network, we select $S$ seeds and agglomerate local regions around them to create super nodes. This then leads to a new network with weighted edges between the $S$ super nodes upon which community detection can be more efficiently applied}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3.1}Objectively Comparing Partitions on Possibly Different Scales}{71}{subsubsection.3.2.3.1}}
\citation{tiagosbm}
\citation{jungirie,kempe}
\citation{CoreHD}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Methods}{72}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Defining seeds}{72}{subsection.3.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces {\bf  Choosing seeds in a synthetic network}. The identification of 20 seeds with the CoreHD algorithm in a network generated from a stochastic block model with 8 communities. Seeds (black nodes) are well distributed across communities.}}{73}{figure.3.3}}
\newlabel{Synth}{{3.3}{73}{{\bf Choosing seeds in a synthetic network}. The identification of 20 seeds with the CoreHD algorithm in a network generated from a stochastic block model with 8 communities. Seeds (black nodes) are well distributed across communities}{figure.3.3}{}}
\citation{snapdata}
\citation{newmandata}
\citation{blondel}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Grow Super Nodes Around Seeds}{74}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Create Network of Super Nodes}{74}{subsection.3.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces {\bf  Network data characteristics.} }}{75}{table.3.1}}
\newlabel{table:dataset}{{3.1}{75}{\bf Network data characteristics}{table.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Results}{75}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Overview of experiments}{75}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Normalized mutual information and under segmentation error}{76}{subsection.3.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces {\bf  Schematic of possible partition comparisons.} We outline the types of possible comparisons between partitions generated according to various combinations of network representation and community detection method. According to these comparison rules, we compute normalized mutual information (NMI) between all pairs of networks satisfying the comparison criteria. The colored circles in the schematic represent a single partition generated under the corresponding network representation and community detection algorithm combination. Circles are colored (in each column) by each of the four possible representation/community detection method combinations. In {\bf  A-C}, we outline the types of comparisons we perform in subsequent figures. {\bf  A.} To compare the usefulness of the super node representation in identifying communities retrieved using the full network, we compare pairs of networks with different representations under the same community detection algorithm. {\bf  B.} Due to the stochastic nature of both the Louvain algorithm and SBM fitting, this comparison seeks to quantify partitions generated under the same network representation and method. {\bf  C.} Finally, we consider pairs of partitions generated under the same network representation and different community detection algorithms. }}{77}{figure.3.4}}
\newlabel{concept}{{3.4}{77}{{\bf Schematic of possible partition comparisons.} We outline the types of possible comparisons between partitions generated according to various combinations of network representation and community detection method. According to these comparison rules, we compute normalized mutual information (NMI) between all pairs of networks satisfying the comparison criteria. The colored circles in the schematic represent a single partition generated under the corresponding network representation and community detection algorithm combination. Circles are colored (in each column) by each of the four possible representation/community detection method combinations. In {\bf A-C}, we outline the types of comparisons we perform in subsequent figures. {\bf A.} To compare the usefulness of the super node representation in identifying communities retrieved using the full network, we compare pairs of networks with different representations under the same community detection algorithm. {\bf B.} Due to the stochastic nature of both the Louvain algorithm and SBM fitting, this comparison seeks to quantify partitions generated under the same network representation and method. {\bf C.} Finally, we consider pairs of partitions generated under the same network representation and different community detection algorithms}{figure.3.4}{}}
\citation{tiagosbm}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Run time Analysis}{78}{subsection.3.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces {\bf  Super Node Quality.} We computed normalized mutual information ({\bf  A.}) and under segmentation error ({\bf  B.}) for networks represented by between 100 and 600 super nodes. Line type and color indicate the community detection algorithm applied (Louvain algorithm or SBM fitting). Each curve indicates the mean across 5 super node representations. The shaded area shows standard deviation. {\bf  A.} Normalized mutual information between the full and super node representations of networks [i.e. NMI$({\bf  z}^{Full},{\bf  z}^{SN})$]. A network representation with more super nodes. generally increases the NMI between full network and super node network representations. Horizontal lines indicate the mean pairwise NMI between 10 runs of the Louvain algorithm and SBM result on the full network (pink and gold, respectively). Given the high variability between multiple runs of the same algorithm on the full network, adding more super nodes can only improve the NMI between the full and super node representation to the observed level of similarity observed between algorithm runs. {\bf  B.} The log under segmentation error for super node representations. Defining a super node representation with more super nodes generally decreases the under segmentation error. }}{79}{figure.3.5}}
\newlabel{FigQuality}{{3.5}{79}{{\bf Super Node Quality.} We computed normalized mutual information ({\bf A.}) and under segmentation error ({\bf B.}) for networks represented by between 100 and 600 super nodes. Line type and color indicate the community detection algorithm applied (Louvain algorithm or SBM fitting). Each curve indicates the mean across 5 super node representations. The shaded area shows standard deviation. {\bf A.} Normalized mutual information between the full and super node representations of networks [i.e. NMI$({\bf z}^{Full},{\bf z}^{SN})$]. A network representation with more super nodes. generally increases the NMI between full network and super node network representations. Horizontal lines indicate the mean pairwise NMI between 10 runs of the Louvain algorithm and SBM result on the full network (pink and gold, respectively). Given the high variability between multiple runs of the same algorithm on the full network, adding more super nodes can only improve the NMI between the full and super node representation to the observed level of similarity observed between algorithm runs. {\bf B.} The log under segmentation error for super node representations. Defining a super node representation with more super nodes generally decreases the under segmentation error}{figure.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces {\bf  Runtimes}. We compare community detection runtimes (in seconds) with the Louvain algorithm and by fitting an SBM on the full networks and super node representations for the 9 data sets. {\bf  (A.)} Louvain on the full network. {\bf  (B.)} Louvain on the super nodes. {\bf  (C.)} SBM on the full network. {\bf  (D.)} SBM on the super nodes.}}{80}{figure.3.6}}
\newlabel{RT}{{3.6}{80}{{\bf Runtimes}. We compare community detection runtimes (in seconds) with the Louvain algorithm and by fitting an SBM on the full networks and super node representations for the 9 data sets. {\bf (A.)} Louvain on the full network. {\bf (B.)} Louvain on the super nodes. {\bf (C.)} SBM on the full network. {\bf (D.)} SBM on the super nodes}{figure.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Quantifying variability across algorithm runs}{80}{subsection.3.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces {\bf  Quantifying partition variability.} For each of the 9 networks, we obtained 10 different partitions by the Louvain algorithm and 10 different SBM fits under the default ({\bf  A.}) and matched settings ({\bf  B.}). To assess the similarity between partitions within and between a community detection algorithm in networks under the the super node representation, we computed pairwise normalized mutual information (NMI) as a function of the number of super nodes. The pink and blue curves show the mean pairwise normalized mutual information between all pairs of 10 partitions under Louvain and SBM fitting, respectively. The gold curves compare pairs of partitions under different methods. Shaded area denotes standard deviation. Horizontal lines indicates the mean pairwise NMI between partitions under the full network representation for within Louvain and SBM partition comparison (pink and blue, respectively) and between Louvain and SBM partition comparison (gold). Overall, the super node representation is useful for reducing the disparity between the partitions obtained under different methods.}}{81}{figure.3.7}}
\newlabel{VAR}{{3.7}{81}{{\bf Quantifying partition variability.} For each of the 9 networks, we obtained 10 different partitions by the Louvain algorithm and 10 different SBM fits under the default ({\bf A.}) and matched settings ({\bf B.}). To assess the similarity between partitions within and between a community detection algorithm in networks under the the super node representation, we computed pairwise normalized mutual information (NMI) as a function of the number of super nodes. The pink and blue curves show the mean pairwise normalized mutual information between all pairs of 10 partitions under Louvain and SBM fitting, respectively. The gold curves compare pairs of partitions under different methods. Shaded area denotes standard deviation. Horizontal lines indicates the mean pairwise NMI between partitions under the full network representation for within Louvain and SBM partition comparison (pink and blue, respectively) and between Louvain and SBM partition comparison (gold). Overall, the super node representation is useful for reducing the disparity between the partitions obtained under different methods}{figure.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Neighborhood agreement}{82}{subsection.3.4.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces {\bf  Agreement of community assignments with local connectivity}. We study how consistent partitions are within local neighborhood regions of the network by examining how well a node's neighbors (for various order neighborhoods) can be used to predict its community assignment, under some community partition ${\bf  z}$. For each community in a partition, we give a binary prediction of whether a node is assigned to that community, based on probabilities we compute for a node from its neighbors. Sweeping the parameter $p$ that sets the probability required for a node to be assigned to a community, we compute ROC curves for each community and report the minimum AUC value observed. Panels {\bf  A-D} show minimum AUC values observed as a function of neighborhood order for communities obtained from the full networks and super node representations by Louvain and by SBM. Line color indicates network and line type indicates communities obtained from the matched and default parameters used by the algorithms on the full networks. Panels {\bf  E-H} visualize the communities obtained in the As22 data on the full network (default parameters) and super node representation (SN) under Louvain and SBM, with node colors indicating community memberships. }}{83}{figure.3.8}}
\newlabel{Viz}{{3.8}{83}{{\bf Agreement of community assignments with local connectivity}. We study how consistent partitions are within local neighborhood regions of the network by examining how well a node's neighbors (for various order neighborhoods) can be used to predict its community assignment, under some community partition ${\bf z}$. For each community in a partition, we give a binary prediction of whether a node is assigned to that community, based on probabilities we compute for a node from its neighbors. Sweeping the parameter $p$ that sets the probability required for a node to be assigned to a community, we compute ROC curves for each community and report the minimum AUC value observed. Panels {\bf A-D} show minimum AUC values observed as a function of neighborhood order for communities obtained from the full networks and super node representations by Louvain and by SBM. Line color indicates network and line type indicates communities obtained from the matched and default parameters used by the algorithms on the full networks. Panels {\bf E-H} visualize the communities obtained in the As22 data on the full network (default parameters) and super node representation (SN) under Louvain and SBM, with node colors indicating community memberships}{figure.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Conclusion and Future Work}{84}{section.3.5}}
\citation{hric,peel2017ground,jureTruth}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Stochastic Block Models with Multiple Continuous Attributes}{85}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{cesna,clauset,ilouvain,hric,peel2017ground}
\citation{ilouvain}
\citation{blondel}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{86}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Related work in attributed networks}{86}{subsection.4.1.1}}
\citation{clauset,hric,peel2017ground,cesna}
\citation{cesna}
\citation{originalSBM}
\citation{clauset}
\citation{hric}
\citation{peel2017ground}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Methods}{88}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Attributed SBM model definition}{88}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1.1}Objective}{88}{subsubsection.4.2.1.1}}
\citation{dempster}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces {\bf  Modeling community membership in terms of attributes and connectivity}. Node-to-community assignments specified by ${\bf  Z}$ are determined in terms of adjacency matrix information, ${\bf  A}$ and attribute matrix information, ${\bf  X}$. ${\bf  A}$ and ${\bf  X}$ are assumed by be generated from a stochastic block model and a mixture of multivariate Gaussian distributions, parameterized by ${\boldsymbol  \theta }$ and ${\boldsymbol  \Psi }$, respectively. }}{89}{figure.4.1}}
\newlabel{fig:graphical_model}{{4.1}{89}{{\bf Modeling community membership in terms of attributes and connectivity}. Node-to-community assignments specified by ${\bf Z}$ are determined in terms of adjacency matrix information, ${\bf A}$ and attribute matrix information, ${\bf X}$. ${\bf A}$ and ${\bf X}$ are assumed by be generated from a stochastic block model and a mixture of multivariate Gaussian distributions, parameterized by ${\boldsymbol \theta }$ and ${\boldsymbol \Psi }$, respectively}{figure.4.1}{}}
\newlabel{eqn:likelihood_decomposition}{{4.1}{89}{Objective\relax }{equation.4.2.1}{}}
\newlabel{post}{{4.3}{89}{Objective\relax }{equation.4.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1.2}Attribute Likelihood}{89}{subsubsection.4.2.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1.3}Adjacency Matrix Likelihood}{90}{subsubsection.4.2.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1.4}Inference}{90}{subsubsection.4.2.1.4}}
\citation{dudin}
\citation{blondel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1.5}Initialization}{91}{subsubsection.4.2.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Synthetic Data Results}{91}{subsection.4.2.2}}
\citation{commdeccompare}
\citation{decelle2011inference,taylor2015enhanced}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces {\bf  Synthetic Example.} We generated a synthetic network with $N=200$ nodes, $K=4$ communities and an 8-dimensional multivariate Gaussian for each community. {\bf  A.} A visualization of the adjacency matrix for this network where a black dot indicates an edge. We observe that there is an assortative block structure (blocks on the diagonal), but there are also many edges between communities making the true community structure using only connectivity harder to detect. {\bf  B.} We performed PCA on the $N \times p$ attribute array and plotted each of the $N$ nodes in two dimensions. Points are colored by their true community assignments, ${\bf  z}$. Clustering the nodes according to only connectivity, only attributes, and with the attributed SBM, we quantified the partition accuracy with normalized mutual information, yielding $\text  {NMI}({\bf  z},\{{\bf  z}^{\text  {connectivity}}, {\bf  z}^{\text  {attributes}},{\bf  z}^{\text  {attribute sbm}}\})=\{0.65,0.68,0.83\}$.}}{93}{figure.4.2}}
\newlabel{AttFig2}{{4.2}{93}{{\bf Synthetic Example.} We generated a synthetic network with $N=200$ nodes, $K=4$ communities and an 8-dimensional multivariate Gaussian for each community. {\bf A.} A visualization of the adjacency matrix for this network where a black dot indicates an edge. We observe that there is an assortative block structure (blocks on the diagonal), but there are also many edges between communities making the true community structure using only connectivity harder to detect. {\bf B.} We performed PCA on the $N \times p$ attribute array and plotted each of the $N$ nodes in two dimensions. Points are colored by their true community assignments, ${\bf z}$. Clustering the nodes according to only connectivity, only attributes, and with the attributed SBM, we quantified the partition accuracy with normalized mutual information, yielding $\text {NMI}({\bf z},\{{\bf z}^{\text {connectivity}}, {\bf z}^{\text {attributes}},{\bf z}^{\text {attribute sbm}}\})=\{0.65,0.68,0.83\}$}{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces {\bf  Detectability Analysis in Synthetic Example.} To understand how attribute information can be combined with connectivity to assign nodes to communities accurately, we generated synthetic networks for within-probabilities of $p_{in}$ between 0.05 and 0.3 with corresponding $p_{out}$ or between-community probabilities such that the mean degree of the network was 20. For each of these synthetic networks, we used the attributes from the analysis in figure 2 to fit the attributed SBM. Here, we plot the correctness of the node-to-community assignment with normalized mutual information using the partition obtained from regular SBM (blue) and the partition under the attributed SBM model fit (pink). For each combination of $p_{in}$ and $p_{out}$, we generated 10 networks and hence the bands around the points denote standard deviation. Incorporating attributes with the attributes stochastic block model improves results, particularly near and below the detectability limit, and appears to smooth out the sharp phase transition. }}{94}{figure.4.3}}
\newlabel{AttFig3}{{4.3}{94}{{\bf Detectability Analysis in Synthetic Example.} To understand how attribute information can be combined with connectivity to assign nodes to communities accurately, we generated synthetic networks for within-probabilities of $p_{in}$ between 0.05 and 0.3 with corresponding $p_{out}$ or between-community probabilities such that the mean degree of the network was 20. For each of these synthetic networks, we used the attributes from the analysis in figure 2 to fit the attributed SBM. Here, we plot the correctness of the node-to-community assignment with normalized mutual information using the partition obtained from regular SBM (blue) and the partition under the attributed SBM model fit (pink). For each combination of $p_{in}$ and $p_{out}$, we generated 10 networks and hence the bands around the points denote standard deviation. Incorporating attributes with the attributes stochastic block model improves results, particularly near and below the detectability limit, and appears to smooth out the sharp phase transition}{figure.4.3}{}}
\citation{linkPredReview}
\citation{collabFilterReview}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Attributed SBM for Link Prediction and Collaborative Filtering}{95}{subsection.4.2.3}}
\citation{linkComm}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3.1}Link Prediction Experiments}{96}{subsubsection.4.2.3.1}}
\citation{collabComm}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3.2}Collaborative Filtering Experiments}{97}{subsubsection.4.2.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Results in biological networks}{97}{subsection.4.2.4}}
\citation{microbiomedata}
\citation{dudin}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.1}Microbiome Subject Similarity Results}{98}{subsubsection.4.2.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces {\bf  Microbiome subject similarity network:} A visualization of the 121 node microbiome subject similarity network with nodes colored by the partition using the classic ({\bf  A.}) and attributed ({\bf  B.}) stochastic block model. {\bf  A.} Fitting the classic stochastic block model to the network, 7 communities were identified. {\bf  B.} Fitting the attributed stochastic block model to the network with the attributes being the first 5 principle components of each subject's OTU count vector (metagenomic profile), 6 communities were identified. Incorporating attributes in inferring this partition removed some of the noise in the partition on the network, specifically in the mixed purple community in the left of {\bf  A.}}}{99}{figure.4.4}}
\newlabel{AttFig4}{{4.4}{99}{{\bf Microbiome subject similarity network:} A visualization of the 121 node microbiome subject similarity network with nodes colored by the partition using the classic ({\bf A.}) and attributed ({\bf B.}) stochastic block model. {\bf A.} Fitting the classic stochastic block model to the network, 7 communities were identified. {\bf B.} Fitting the attributed stochastic block model to the network with the attributes being the first 5 principle components of each subject's OTU count vector (metagenomic profile), 6 communities were identified. Incorporating attributes in inferring this partition removed some of the noise in the partition on the network, specifically in the mixed purple community in the left of {\bf A.}\relax }{figure.4.4}{}}
\citation{bonacci}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces {\bf  Link Prediction on the microbiome subject similarity network:} The results for link prediction on the microbiome subject similarity network for the attributed SBM, Jaccard, Adamic-Adar and preferential attachment methods. The corresponding AUC values for these methods, respectively are, 0.71, 0.69, 0.69, and 0.62.}}{100}{figure.4.5}}
\newlabel{AttFig5}{{4.5}{100}{{\bf Link Prediction on the microbiome subject similarity network:} The results for link prediction on the microbiome subject similarity network for the attributed SBM, Jaccard, Adamic-Adar and preferential attachment methods. The corresponding AUC values for these methods, respectively are, 0.71, 0.69, 0.69, and 0.62}{figure.4.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.2}Protein Interaction Network Results}{100}{subsubsection.4.2.4.2}}
\citation{bonacci}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces {\bf  Collaborative Filtering Accuracy in Microbiome Subject Similarity Network}: For each of the 121 nodes, we fit a model to the remaining 120 node network and given the node's closest neighbors (based on network connectivity) sought to predict its 5-dimensional attribute vector. The reported error is the relative error $\mathcal  {E}$ between the difference between the true attribute vector (${\bf  x}_{i}$) and its predicted attribute vector (${\mathaccentV {hat}05E{\bf  x}}_{i}$). The mean error in ${\bf  x}_{i}$ is 0.21, as opposed to the neighbor average and weighted neighbor averages, having errors of 0.26 and 0.27, respectively. }}{101}{figure.4.6}}
\newlabel{AttFig6}{{4.6}{101}{{\bf Collaborative Filtering Accuracy in Microbiome Subject Similarity Network}: For each of the 121 nodes, we fit a model to the remaining 120 node network and given the node's closest neighbors (based on network connectivity) sought to predict its 5-dimensional attribute vector. The reported error is the relative error $\mathcal {E}$ between the difference between the true attribute vector (${\bf x}_{i}$) and its predicted attribute vector (${\hat {\bf x}}_{i}$). The mean error in ${\bf x}_{i}$ is 0.21, as opposed to the neighbor average and weighted neighbor averages, having errors of 0.26 and 0.27, respectively}{figure.4.6}{}}
\citation{dudin}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces {\bf  Protein interaction network.} We visualize the 82 node protein interaction network under the classic stochastic block model {\bf  A.} and the attributed stochastic block model {\bf  B.} In both networks, nodes are colored by their community assignment and the node shape indicates whether the modification status increased (square) or decreased. {\bf  A.} Nodes colored according to the community partition under the stochastic block model. Nodes are assigned to one of five communities. {\bf  B.} Nodes are colored to the community partition under one of nine communities.}}{102}{figure.4.7}}
\newlabel{AttFig7}{{4.7}{102}{{\bf Protein interaction network.} We visualize the 82 node protein interaction network under the classic stochastic block model {\bf A.} and the attributed stochastic block model {\bf B.} In both networks, nodes are colored by their community assignment and the node shape indicates whether the modification status increased (square) or decreased. {\bf A.} Nodes colored according to the community partition under the stochastic block model. Nodes are assigned to one of five communities. {\bf B.} Nodes are colored to the community partition under one of nine communities}{figure.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces {\bf  Community entropies in the protein interaction network.} We studied the entropy of the 2 class and 6 class classifications of the nodes in {\bf  A.} and {\bf  B.}, respectively under the classic SBM (black) and attributed SBM (purple) partitions. For ${\bf  A.-B.}$ the horizontal axis denotes the community index for the particular partition. Nodes belonged to 1 of 5 communities under the classic SBM and belong to 1 of 9 communities with the attributed SBM. Incorporating attributes under both classifications succeeds in breaking up a high entropy community (5) from the classic SBM partition to lower entropy communities in the attributed SBM partition. }}{103}{figure.4.8}}
\newlabel{entropyFig}{{4.8}{103}{{\bf Community entropies in the protein interaction network.} We studied the entropy of the 2 class and 6 class classifications of the nodes in {\bf A.} and {\bf B.}, respectively under the classic SBM (black) and attributed SBM (purple) partitions. For ${\bf A.-B.}$ the horizontal axis denotes the community index for the particular partition. Nodes belonged to 1 of 5 communities under the classic SBM and belong to 1 of 9 communities with the attributed SBM. Incorporating attributes under both classifications succeeds in breaking up a high entropy community (5) from the classic SBM partition to lower entropy communities in the attributed SBM partition}{figure.4.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces {\bf  Link Prediction in the protein interaction network}. Performing link prediction using the attributed SBM, Jaccard, Adamic Adar, and preferential attachment. The corresponding AUC curves for these methods were 0.61, 0.58, 0.58, and 0.51, respectively.}}{104}{figure.4.9}}
\newlabel{AttFig9}{{4.9}{104}{{\bf Link Prediction in the protein interaction network}. Performing link prediction using the attributed SBM, Jaccard, Adamic Adar, and preferential attachment. The corresponding AUC curves for these methods were 0.61, 0.58, 0.58, and 0.51, respectively}{figure.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces {\bf  Collaborative filtering in the protein interaction network}. For each of the 82 nodes, we fit a model to the remaining 81 node network and given the node's closest neighbors (based on network connectivity) sought to predict its 6-dimensional attribute vector. The reported error is the relative error $\mathcal  {E}$ between the difference between the true attribute vector (${\bf  x}_{i}$) and its predicted attribute vector (${\mathaccentV {hat}05E{\bf  x}}_{i}$). The mean error in ${\bf  x}_{i}$ using the attributed SBM is 0.21, as opposed to the neighbor average error where it is 0.48. }}{105}{figure.4.10}}
\newlabel{collabprotein}{{4.10}{105}{{\bf Collaborative filtering in the protein interaction network}. For each of the 82 nodes, we fit a model to the remaining 81 node network and given the node's closest neighbors (based on network connectivity) sought to predict its 6-dimensional attribute vector. The reported error is the relative error $\mathcal {E}$ between the difference between the true attribute vector (${\bf x}_{i}$) and its predicted attribute vector (${\hat {\bf x}}_{i}$). The mean error in ${\bf x}_{i}$ using the attributed SBM is 0.21, as opposed to the neighbor average error where it is 0.48}{figure.4.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Conclusion}{105}{section.4.3}}
\citation{hric,peel2017ground,ilouvain,cesna,clauset,perozziAttribute}
\citation{clauset}
\citation{peel2017ground}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Testing the Alignment of Node Attributes with Network Structure}{107}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{107}{section.5.1}}
\citation{hric}
\citation{peel2017ground}
\citation{clauset}
\citation{clauset}
\citation{peel2017ground}
\citation{hric}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Attributed Network Community Detection Methods}{108}{subsection.5.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1.1}Probabilistic approaches}{108}{subsubsection.5.1.1.1}}
\citation{cesna}
\citation{newmanmodularity}
\citation{blondel}
\citation{ilouvain}
\citation{perozziAttribute}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1.2}Quality function maximization}{109}{subsubsection.5.1.1.2}}
\citation{LabProp}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Methods}{110}{section.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces {\bf  Overview of the method}. Our test first labels the nodes according to attribute information, $\mathaccentV {tilde}07E{{\bf  z}}$. Then in a collection of $T$ trials, a sample of $l$ nodes is treated as labeled, according to $\mathaccentV {tilde}07E{{\bf  z}}$. In each trial, a label propagation task is performed to predict the probability distribution over communities for the unlabeled $N-l$ nodes. The entropy of the node-to-community assignment probabilities is used as an estimate of how well the attributes align with connectivity. Also in each trial, $\mathaccentV {tilde}07E{{\bf  z}}$ is permuted and subjected to the label propagation task to compute a `null' entropy value. After repeating this process in $T$ trials, the empirical $p$-value is calculated based on the overlap between the null entropy distribution and the empirical entropy distribution.}}{111}{figure.5.1}}
\newlabel{Overview}{{5.1}{111}{{\bf Overview of the method}. Our test first labels the nodes according to attribute information, $\tilde {{\bf z}}$. Then in a collection of $T$ trials, a sample of $l$ nodes is treated as labeled, according to $\tilde {{\bf z}}$. In each trial, a label propagation task is performed to predict the probability distribution over communities for the unlabeled $N-l$ nodes. The entropy of the node-to-community assignment probabilities is used as an estimate of how well the attributes align with connectivity. Also in each trial, $\tilde {{\bf z}}$ is permuted and subjected to the label propagation task to compute a `null' entropy value. After repeating this process in $T$ trials, the empirical $p$-value is calculated based on the overlap between the null entropy distribution and the empirical entropy distribution}{figure.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Notation}{111}{subsection.5.2.1}}
\citation{learning2}
\citation{learning2}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Classifying Nodes}{112}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Sampling Nodes and Creating Entropy Distributions}{112}{subsection.5.2.3}}
\newlabel{FP}{{5.1}{113}{Sampling Nodes and Creating Entropy Distributions\relax }{equation.5.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Computing the empirical $p$-value}{113}{subsection.5.2.4}}
\citation{TwoD}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results}{114}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Synthetic Examples}{114}{subsection.5.3.1}}
\citation{peel2017ground}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces {\bf  Properties of the empirical $p$-value.} To understand the properties of our empirical $p$-value, we generated a synthetic network, ${\bf  A}$ from an SBM with $N=200$ nodes, $K=4$ communities. The vector of continuous attributes for a node $i$, ($X_{i}$) was drawn from a multivariate Gaussian distribution parameterized by its community assignment ($z_{i}$) or $\{{\boldsymbol  \mu }_{z_{i}}, {\boldsymbol  \Sigma }_{z_{i}}\}$. In these experiments, we permuted varying fractions of $\mathaccentV {tilde}07E{{\bf  z}}$ and observed the effects on entropy and empirical $p$-value. {\bf  A}. We used tSNE to visualize the two dimensional projection of the 200 nodes. For the most part, members of the same community cluster together. {\bf  B.} We plotted the empirical $p$-value as a function of the proportion of labels permuted and observed decreased statistical significance (increased empirical $p$-value) with an increasing proportion of permuted labels. {\bf  C.} We plotted the empirical $p$-value as a function of the mean entropy ($\mathcal  {E}$) across $T=1000$ trials used to generate the entropy distributions for each experiment. Increased entropy corresponding to a larger proportion of $\mathaccentV {tilde}07E{\bf  z}$ permuted leads to a decreased $p$-value.}}{115}{figure.5.2}}
\newlabel{Align:Syn1}{{5.2}{115}{{\bf Properties of the empirical $p$-value.} To understand the properties of our empirical $p$-value, we generated a synthetic network, ${\bf A}$ from an SBM with $N=200$ nodes, $K=4$ communities. The vector of continuous attributes for a node $i$, ($X_{i}$) was drawn from a multivariate Gaussian distribution parameterized by its community assignment ($z_{i}$) or $\{{\boldsymbol \mu }_{z_{i}}, {\boldsymbol \Sigma }_{z_{i}}\}$. In these experiments, we permuted varying fractions of $\tilde {{\bf z}}$ and observed the effects on entropy and empirical $p$-value. {\bf A}. We used tSNE to visualize the two dimensional projection of the 200 nodes. For the most part, members of the same community cluster together. {\bf B.} We plotted the empirical $p$-value as a function of the proportion of labels permuted and observed decreased statistical significance (increased empirical $p$-value) with an increasing proportion of permuted labels. {\bf C.} We plotted the empirical $p$-value as a function of the mean entropy ($\mathcal {E}$) across $T=1000$ trials used to generate the entropy distributions for each experiment. Increased entropy corresponding to a larger proportion of $\tilde {\bf z}$ permuted leads to a decreased $p$-value}{figure.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1.1}Comparison to BESTest}{115}{subsubsection.5.3.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1.2}Strength of community structure}{116}{subsubsection.5.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces {\bf  Comparison with BESTest.} We sought to understand the relationship between our empirical $p$-value and that computed according to BESTest. To study this, we used the same experiment described in Figure \ref  {Align:Syn1}, where we varied the proportion of permuted labels from $\mathaccentV {tilde}07E{\bf  z}$. We denote our empirical $p$-value by `LP empirical $p$-value. {\bf  A.} We plotted the BESTest empirical $p$-value against our LP empirical $p$-value. {\bf  B.} We plotted the BESTest empirical $p$-value as a function of the BESTest entropy. BESTest gives a significant empirical $p$-value for a much wider range of entropy levels than our test. {\bf  C.} The experiments produced a wide range of entropies under BESTest, which are captured by corresponding differences in our empirical $p$-value. {\bf  D.} We compared the BESTest approach to computing entropy to our LP method and observed a high correlation between these entropy measures $(r=0.95)$. }}{117}{figure.5.3}}
\newlabel{BestCompare}{{5.3}{117}{{\bf Comparison with BESTest.} We sought to understand the relationship between our empirical $p$-value and that computed according to BESTest. To study this, we used the same experiment described in Figure \ref {Align:Syn1}, where we varied the proportion of permuted labels from $\tilde {\bf z}$. We denote our empirical $p$-value by `LP empirical $p$-value. {\bf A.} We plotted the BESTest empirical $p$-value against our LP empirical $p$-value. {\bf B.} We plotted the BESTest empirical $p$-value as a function of the BESTest entropy. BESTest gives a significant empirical $p$-value for a much wider range of entropy levels than our test. {\bf C.} The experiments produced a wide range of entropies under BESTest, which are captured by corresponding differences in our empirical $p$-value. {\bf D.} We compared the BESTest approach to computing entropy to our LP method and observed a high correlation between these entropy measures $(r=0.95)$}{figure.5.3}{}}
\citation{cytof}
\citation{wong2015}
\citation{cytofkit}
\citation{phenoGraph}
\citation{blondel}
\citation{cytofkit}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces {\bf  Analysis of the strength of structural communities.} To understand the effect of network structure on our test, we generated synthetic networks from stochastic block models with various $p_{in}$ (within-community) and $p_{out}$ (between-community) parameters. Networks were generated with $p_{in}$ varying between 0.05 and 0.45 and we chose a corresponding $p_{out}$ such that the mean degree was 30. We used $p_{in}/p_{out}$ as a proxy for the strength of community, with a higher value of this ratio indicating a stronger community structure with more within-community edges and fewer between community edges. For each $p_{in}$, $p_{out}$ combination, we generated 10 synthetic network realizations. {\bf  A.} We plotted the relationship between our LP entropy and $p_{in}/p_{out}$. The shaded area denotes standard deviation of the mean entropy over the 10 networks for each $p_{in}$, $p_{out}$ combination. {\bf  B.} Similar to ({\bf  A.}), we plotted the mean empirical $p$-value over the $T=1000$ trials used to generate the entropy distributions, $\mathcal  {E}$ and $\mathcal  {E}_{\text  {perm}}$. For large $p_{in}/p_{out}$, the empirical $p$-value became more significant. The shaded area denotes standard deviation of empirical $p$-value over the 10 networks for each $p_{in}$, $p_{out}$ combination. {\bf  C.} Finally, we plotted the relationship between the mean entropy over the $T$=1000 trials, $\mathcal  {E}$ and the empirical $p$-value. These values are strongly correlated with $r=0.91$. }}{118}{figure.5.4}}
\newlabel{Pin}{{5.4}{118}{{\bf Analysis of the strength of structural communities.} To understand the effect of network structure on our test, we generated synthetic networks from stochastic block models with various $p_{in}$ (within-community) and $p_{out}$ (between-community) parameters. Networks were generated with $p_{in}$ varying between 0.05 and 0.45 and we chose a corresponding $p_{out}$ such that the mean degree was 30. We used $p_{in}/p_{out}$ as a proxy for the strength of community, with a higher value of this ratio indicating a stronger community structure with more within-community edges and fewer between community edges. For each $p_{in}$, $p_{out}$ combination, we generated 10 synthetic network realizations. {\bf A.} We plotted the relationship between our LP entropy and $p_{in}/p_{out}$. The shaded area denotes standard deviation of the mean entropy over the 10 networks for each $p_{in}$, $p_{out}$ combination. {\bf B.} Similar to ({\bf A.}), we plotted the mean empirical $p$-value over the $T=1000$ trials used to generate the entropy distributions, $\mathcal {E}$ and $\mathcal {E}_{\text {perm}}$. For large $p_{in}/p_{out}$, the empirical $p$-value became more significant. The shaded area denotes standard deviation of empirical $p$-value over the 10 networks for each $p_{in}$, $p_{out}$ combination. {\bf C.} Finally, we plotted the relationship between the mean entropy over the $T$=1000 trials, $\mathcal {E}$ and the empirical $p$-value. These values are strongly correlated with $r=0.91$}{figure.5.4}{}}
\citation{commdeccompare}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Mass Cytometry Network Example}{119}{subsection.5.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces {\bf  Alignment of markers with communities.} We considered each of the possible 51 features in the single cell data and their potential to be used as markers of particular inferred cellular phenotypes. We identified 10 communities (or inferred phenotypes) under the Louvain algorithm, producing a partition of the network, ${\bf  z}$. We then created a partition, $\mathaccentV {tilde}07E{\bf  z}$ from each attribute in isolation. For each attribute and its induced partition of the nodes, $\mathaccentV {tilde}07E{\bf  z}$, normalized mutual information (NMI) was used to measure the discriminative power of the marker in distinguishing network communities, or NMI($ \mathaccentV {tilde}07E{\bf  z},{\bf  z}$). We expected that our $p$-value should align with this NMI measure in that markers leading to high NMI between the induced $\mathaccentV {tilde}07E{\bf  z}$ and ${\bf  z}$ should have more significant $p$-values. {\bf  A.} We used a histogram to visualize the distribution of NMI values across the 51 possible markers, with many of them leading to low NMI (between 0 and 0.1). {\bf  B.} Similar to ({\bf  A.}), we visualized the empirical $p$-value for the 51 possible markers. {\bf  C.} We compared the relationship between the empirical $p$-value (vertical axis) and NMI($ \mathaccentV {tilde}07E{\bf  z},{\bf  z}$) (horizontal axis) across the 51 possible markers. As expected, we observed these quantities to be anti-correlated in that more significant (lower) empirical $p$-values were obtained for higher values of NMI($ \mathaccentV {tilde}07E{\bf  z},{\bf  z}$).}}{120}{figure.5.5}}
\newlabel{MarkerDist}{{5.5}{120}{{\bf Alignment of markers with communities.} We considered each of the possible 51 features in the single cell data and their potential to be used as markers of particular inferred cellular phenotypes. We identified 10 communities (or inferred phenotypes) under the Louvain algorithm, producing a partition of the network, ${\bf z}$. We then created a partition, $\tilde {\bf z}$ from each attribute in isolation. For each attribute and its induced partition of the nodes, $\tilde {\bf z}$, normalized mutual information (NMI) was used to measure the discriminative power of the marker in distinguishing network communities, or NMI($ \tilde {\bf z},{\bf z}$). We expected that our $p$-value should align with this NMI measure in that markers leading to high NMI between the induced $\tilde {\bf z}$ and ${\bf z}$ should have more significant $p$-values. {\bf A.} We used a histogram to visualize the distribution of NMI values across the 51 possible markers, with many of them leading to low NMI (between 0 and 0.1). {\bf B.} Similar to ({\bf A.}), we visualized the empirical $p$-value for the 51 possible markers. {\bf C.} We compared the relationship between the empirical $p$-value (vertical axis) and NMI($ \tilde {\bf z},{\bf z}$) (horizontal axis) across the 51 possible markers. As expected, we observed these quantities to be anti-correlated in that more significant (lower) empirical $p$-values were obtained for higher values of NMI($ \tilde {\bf z},{\bf z}$)}{figure.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces {\bf  Validation with well and poorly aligned markers}. We used two markers with different correlation strength with communities as another validation of the computed entropy under label propagations. First, we defined a labeling of the nodes, $\mathaccentV {tilde}07E{\bf  z}$ based on marker (Rh103)Di$<\text  {BC103}>$ that did not vary across communities in its expression, and hence not discriminate between the communities. {\bf  A.} We visualized the distribution of $\mathcal  {E}$ (purple), in comparison to $\mathcal  {E}_{\text  {perm}}$ (gold). Since this marker has low discriminative power, we expected the shown overlap between $\mathcal  {E}$ and $\mathcal  {E}_{\text  {perm}}$. {\bf  B.} We plotted the network of the 1000 single cells and colored nodes by their expression of (Rh103)Di$<\text  {BC103}>$, with lighter colors indicating higher expression. It is difficult to notice clustering in this network between cells with similar expression values. {\bf  C.} Conversely to the result shown in ({\bf  A.}), we chose a marker with high discriminative power, (Nd146)Di$<\text  {CD}8>$. Again, we show the distribution of $\mathcal  {E}$ (purple), in comparison to $\mathcal  {E}_{\text  {perm}}$ (gold). Since this marker has good discriminative power, $\mathcal  {E}$ and $\mathcal  {E}_{\text  {perm}}$ do not overlap. {\bf  D.} We plotted the network of single cells, with nodes colored according to the intensity of (Nd146)Di$<\text  {CD}8>$, with lighter colors indicating higher expression.}}{121}{figure.5.6}}
\newlabel{MassCy}{{5.6}{121}{{\bf Validation with well and poorly aligned markers}. We used two markers with different correlation strength with communities as another validation of the computed entropy under label propagations. First, we defined a labeling of the nodes, $\tilde {\bf z}$ based on marker (Rh103)Di$<\text {BC103}>$ that did not vary across communities in its expression, and hence not discriminate between the communities. {\bf A.} We visualized the distribution of $\mathcal {E}$ (purple), in comparison to $\mathcal {E}_{\text {perm}}$ (gold). Since this marker has low discriminative power, we expected the shown overlap between $\mathcal {E}$ and $\mathcal {E}_{\text {perm}}$. {\bf B.} We plotted the network of the 1000 single cells and colored nodes by their expression of (Rh103)Di$<\text {BC103}>$, with lighter colors indicating higher expression. It is difficult to notice clustering in this network between cells with similar expression values. {\bf C.} Conversely to the result shown in ({\bf A.}), we chose a marker with high discriminative power, (Nd146)Di$<\text {CD}8>$. Again, we show the distribution of $\mathcal {E}$ (purple), in comparison to $\mathcal {E}_{\text {perm}}$ (gold). Since this marker has good discriminative power, $\mathcal {E}$ and $\mathcal {E}_{\text {perm}}$ do not overlap. {\bf D.} We plotted the network of single cells, with nodes colored according to the intensity of (Nd146)Di$<\text {CD}8>$, with lighter colors indicating higher expression}{figure.5.6}{}}
\citation{blondel}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces {\bf  Variation of markers with significant empirical $p$-values across communities}. We computed the empirical $p$-values induced by the partition $\mathaccentV {tilde}07E{\bf  z}$ for each of the 51 markers and looked closely at the top and bottom 5 markers, as inferred through the empirical $p$-value. Since a quality marker in this case is said to be one that induces a labeled of the nodes, $\mathaccentV {tilde}07E{\bf  z}$ similar to the result obtained under ${\bf  z}$, we expect the expression of such a marker to vary across communities. In this plot, we show the expression of each marker as a function of the community index. The family of orange-colored lines correspond to the top 5 predicted markers (according to empirical $p$-value). From all of these lines, the expression varies across communities. Conversely, we plotted the lowest-ranked markers (in terms of empirical $p$-value and their expression is relatively constant across all communities.}}{123}{figure.5.7}}
\newlabel{MarkerRank}{{5.7}{123}{{\bf Variation of markers with significant empirical $p$-values across communities}. We computed the empirical $p$-values induced by the partition $\tilde {\bf z}$ for each of the 51 markers and looked closely at the top and bottom 5 markers, as inferred through the empirical $p$-value. Since a quality marker in this case is said to be one that induces a labeled of the nodes, $\tilde {\bf z}$ similar to the result obtained under ${\bf z}$, we expect the expression of such a marker to vary across communities. In this plot, we show the expression of each marker as a function of the community index. The family of orange-colored lines correspond to the top 5 predicted markers (according to empirical $p$-value). From all of these lines, the expression varies across communities. Conversely, we plotted the lowest-ranked markers (in terms of empirical $p$-value and their expression is relatively constant across all communities}{figure.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Conclusion}{124}{section.5.4}}
\citation{walsh2017}
\citation{boon}
\citation{mutualism}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}A network approach to understanding microbiome disruption in response to acute lung injury}{125}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{125}{section.6.1}}
\citation{miSeq}
\citation{your}
\citation{sparcc}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Data Background}{126}{subsection.6.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Network Analysis Methods}{126}{section.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Creating Networks with SparCC}{126}{subsection.6.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces {\bf  Microbial co-occurence networks for each patient cohort}. We constructed networks with SparCC in the ALI and non-ALI cohort networks (left and right, respectively). Four communities were identified in each network. Nodes are colored by their community assignment.}}{127}{figure.6.1}}
\newlabel{alinet}{{6.1}{127}{{\bf Microbial co-occurence networks for each patient cohort}. We constructed networks with SparCC in the ALI and non-ALI cohort networks (left and right, respectively). Four communities were identified in each network. Nodes are colored by their community assignment}{figure.6.1}{}}
\citation{picrust}
\citation{gini}
\newlabel{my-label}{{6.3.1}{128}{Community overlap between network\relax }{subsection.6.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces {\bf  Comparing Networks in Each Patient Cohort}. We compare the OTUs in each pair of communities in the ALI and No ALI cohort networks. Large overlaps are denoted by pink shading in the table.}}{128}{table.6.1}}
\newlabel{Tab5}{{6.1}{128}{{\bf Comparing Networks in Each Patient Cohort}. We compare the OTUs in each pair of communities in the ALI and No ALI cohort networks. Large overlaps are denoted by pink shading in the table}{table.6.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Results}{128}{section.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Community overlap between network}{128}{subsection.6.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Evaluating functional differences}{128}{subsection.6.3.2}}
\citation{walsh2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Classifying each community according to predicted function}{129}{subsection.6.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Discussion}{129}{section.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces {\bf  Predictive functions for community classification}. We used a set of 328 filtered functions to predict OTU-to-community assignment in the ALI and No ALI networks. Here we show show the functions identified as the most strong predictors for each community in the ALI and No ALI networks (left and right, respectively). Functions with more discriminative ability in classification from the random forest classifier are ranked higher on the list.}}{130}{figure.6.2}}
\newlabel{Function}{{6.2}{130}{{\bf Predictive functions for community classification}. We used a set of 328 filtered functions to predict OTU-to-community assignment in the ALI and No ALI networks. Here we show show the functions identified as the most strong predictors for each community in the ALI and No ALI networks (left and right, respectively). Functions with more discriminative ability in classification from the random forest classifier are ranked higher on the list}{figure.6.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion and Future Work}{131}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Strata Multilayer Stochastic Block Model}{131}{section.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Recap}{131}{subsection.7.1.1}}
\citation{peelChange}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Future Work}{132}{subsection.7.1.2}}
\citation{spectralWalk,laplacian}
\citation{SuperNodeSide}
\citation{supergenomic}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Super Nodes}{133}{section.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Recap}{133}{subsection.7.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Future Work}{134}{subsection.7.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Stochastic Block Models with Multiple Continuous Attributes}{134}{section.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Recap}{134}{subsection.7.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Future Work}{134}{subsection.7.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Testing Alignment of Attributes and Connectivity}{135}{section.7.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Recap}{135}{subsection.7.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Future Work}{136}{subsection.7.4.2}}
\bibstyle{apalike}
\bibdata{dissBib}
\bibcite{snapdata}{{sna}{}{{}}{{}}}
\bibcite{newmandata}{{new}{}{{}}{{}}}
\bibcite{detect25}{{3}{2016}{{Abbe et~al.}}{{}}}
\bibcite{slic}{{4}{2012}{{Achanta et~al.}}{{}}}
\bibcite{nimaFlow}{{5}{2013}{{Aghaeepour et~al.}}{{}}}
\bibcite{immuneClock}{{6}{2017}{{Aghaeepour et~al.}}{{}}}
\bibcite{aicher}{{7}{2014}{{Aicher et~al.}}{{}}}
\bibcite{weightSBM}{{8}{2015}{{Aicher et~al.}}{{}}}
\bibcite{mixMember}{{9}{2008}{{Airoldi et~al.}}{{}}}
\bibcite{deepGenome}{{10}{2015}{{Alipanahi et~al.}}{{}}}
\bibcite{vBayes}{{11}{2000}{{Attias}}{{}}}
\bibcite{moduleMicrobiome}{{12}{2016}{{Baldassano and Bassett}}{{}}}
\bibcite{barbillon}{{13}{2015}{{Barbillon et~al.}}{{}}}
\bibcite{varIntro}{{14}{2007}{{Barry et~al.}}{{}}}
\bibcite{Neuro}{{15}{2011}{{Bassett et~al.}}{{}}}
\bibcite{detect38}{{16}{2011}{{Benaych-Georges and Nadakuditi}}{{}}}
\bibcite{cytof}{{17}{2012}{{Bendall et~al.}}{{}}}
\@writefile{toc}{\contentsline {chapter}{BIBLIOGRAPHY}{137}{section*.4}}
\bibcite{benderCanfield}{{18}{1978}{{Bender and Canfield}}{{}}}
\bibcite{benson2}{{19}{2016}{{Benson et~al.}}{{}}}
\bibcite{betzel}{{20}{2018}{{Betzel et~al.}}{{}}}
\bibcite{blondel}{{21}{2008}{{Blondel et~al.}}{{}}}
\bibcite{boccaletti2014structure}{{22}{2014}{{Boccaletti et~al.}}{{}}}
\bibcite{bonacci}{{23}{2014}{{Bonacci et~al.}}{{}}}
\bibcite{boon}{{24}{2014}{{Boon et~al.}}{{}}}
\bibcite{netensemble}{{25}{2011}{{Brandes et~al.}}{{}}}
\bibcite{structurenetwork}{{26}{2009}{{Brandes et~al.}}{{}}}
\bibcite{browet}{{27}{2011}{{Browet et~al.}}{{}}}
\bibcite{miSeq}{{28}{2012}{{Caporaso et~al.}}{{}}}
\bibcite{spectralWalk}{{29}{2012}{{Chaudhuri et~al.}}{{}}}
\bibcite{cytofkit}{{30}{2016}{{Chen et~al.}}{{}}}
\bibcite{clausethierarchy}{{31}{2007}{{Clauset et~al.}}{{}}}
\bibcite{ilouvain}{{32}{2015}{{Combe et~al.}}{{}}}
\bibcite{genetic}{{33}{2010}{{Costanzo et~al.}}{{}}}
\bibcite{commdeccompare}{{34}{2005a}{{Danon et~al.}}{{}}}
\bibcite{danon}{{35}{2005b}{{Danon et~al.}}{{}}}
\bibcite{dudin}{{36}{2008}{{Daudin et~al.}}{{}}}
\bibcite{systemsImmuno}{{37}{2017}{{Davis et~al.}}{{}}}
\bibcite{manlio2}{{38}{2015a}{{De~Domenico et~al.}}{{}}}
\bibcite{domen}{{39}{2015b}{{De~Domenico et~al.}}{{}}}
\bibcite{manlioMathFoundations}{{40}{2013}{{De~Domenico et~al.}}{{}}}
\bibcite{decelle2011inference}{{41}{2011a}{{Decelle et~al.}}{{}}}
\bibcite{detect23}{{42}{2011b}{{Decelle et~al.}}{{}}}
\bibcite{dempster}{{43}{1977}{{Dempster et~al.}}{{}}}
\bibcite{collabComm}{{44}{2014}{{Deng et~al.}}{{}}}
\bibcite{cocluster}{{45}{2001}{{Dhillon}}{{}}}
\bibcite{dingcluster}{{46}{2014}{{Ding and Schloss}}{{}}}
\bibcite{microbeco}{{47}{2012}{{Faust et~al.}}{{}}}
\bibcite{pluralHom}{{48}{1981}{{Feld}}{{}}}
\bibcite{fortunato}{{49}{2010}{{Fortunato}}{{}}}
\bibcite{fortu2}{{50}{2016}{{Fortunato and Hric}}{{}}}
\bibcite{sparcc}{{51}{2012}{{Friedman and Alm}}{{}}}
\bibcite{detectTemporal}{{52}{2016}{{Ghasemian et~al.}}{{}}}
\bibcite{gilbert}{{53}{2004}{{Gilbert and Levchenko}}{{}}}
\bibcite{girvancommunity}{{54}{2002}{{Girvan and Newman}}{{}}}
\bibcite{gleichpagerank}{{55}{2015}{{Gleich}}{{}}}
\bibcite{socialnetwork}{{56}{2013}{{Greene and Cunningham}}{{}}}
\bibcite{node2vec}{{57}{2016}{{Grover and Leskovec}}{{}}}
\bibcite{guimera2009missing}{{58}{2009}{{Guimer{\`a} and Sales-Pardo}}{{}}}
\bibcite{airoldi}{{59}{2015}{{Han et~al.}}{{}}}
\bibcite{kMean}{{60}{1979}{{Hartigan and Wong}}{{}}}
\bibcite{hric}{{61}{2016}{{Hric et~al.}}{{}}}
\bibcite{detect22}{{62}{2012}{{Hu et~al.}}{{}}}
\bibcite{confusingMesoscopic}{{63}{2015}{{Iacovacci et~al.}}{{}}}
\bibcite{jakk}{{64}{2001}{{Jaakkola}}{{}}}
\bibcite{abby}{{65}{2014}{{Jacobs and Clauset}}{{}}}
\bibcite{jungirie}{{66}{2012}{{Jung et~al.}}{{}}}
\bibcite{degreecorrectSBM}{{67}{2011}{{Karrer and ~}}{{}}}
\bibcite{degreeCorrect}{{68}{2011}{{Karrer and Newman}}{{}}}
\bibcite{kempe}{{69}{2003}{{Kempe et~al.}}{{}}}
\bibcite{kivelamultilayer}{{70}{2014}{{Kivel{\"a} et~al.}}{{}}}
\bibcite{koller}{{71}{2009}{{Koller and Friedman}}{{}}}
\bibcite{homophily}{{72}{2009}{{Kossinets and Watts}}{{}}}
\bibcite{microbiomedata}{{73}{2014}{{Lahti et~al.}}{{}}}
\bibcite{fortu1}{{74}{2009}{{Lancichinetti and Fortunato}}{{}}}
\bibcite{detect20}{{75}{2011}{{Lancichinetti and Fortunato}}{{}}}
\bibcite{picrust}{{76}{2013}{{Langille et~al.}}{{}}}
\bibcite{larremoreparasite}{{77}{2013}{{Larremore et~al.}}{{}}}
\bibcite{LA}{{78}{2011}{{Latouche et~al.}}{{}}}
\bibcite{networkMicrobiome}{{79}{2017}{{Layeghifard et~al.}}{{}}}
\bibcite{leskoveccommunity}{{80}{2009}{{Leskovec et~al.}}{{}}}
\bibcite{mutualism}{{81}{2008}{{Leung and Poulin}}{{}}}
\bibcite{phenoGraph}{{82}{2015}{{Levine et~al.}}{{}}}
\bibcite{machineGenomics}{{83}{2015}{{Libbrecht and Noble}}{{}}}
\bibcite{supergenomic}{{84}{2014}{{Lisewski et~al.}}{{}}}
\bibcite{structural}{{85}{1971}{{Lorrain and White}}{{}}}
\bibcite{TwoD}{{86}{2008}{{Maaten and Hinton}}{{}}}
\bibcite{biclustering}{{87}{2004}{{Madeira and Oliveira}}{{}}}
\bibcite{intraTumor}{{88}{2012}{{Marusyk et~al.}}{{}}}
\bibcite{gini}{{89}{2009}{{Menze et~al.}}{{}}}
\bibcite{laplacian}{{90}{1994}{{Merris}}{{}}}
\bibcite{hierarchicalmod}{{91}{2009}{{Meunier et~al.}}{{}}}
\bibcite{word2Vec}{{92}{2013}{{Mikolov et~al.}}{{}}}
\bibcite{muchacommunity}{{93}{2010a}{{Mucha et~al.}}{{}}}
\bibcite{muchamultislice}{{94}{2010b}{{Mucha et~al.}}{{}}}
\bibcite{belief}{{95}{1999}{{Murphy et~al.}}{{}}}
\bibcite{detect24}{{96}{2012}{{Nadakuditi and Newman}}{{}}}
\bibcite{detect39}{{97}{2013}{{Nadakuditi and Newman}}{{}}}
\bibcite{newmanAssort}{{98}{2002}{{Newman}}{{}}}
\bibcite{newman2006modularity}{{99}{2006a}{{Newman}}{{}}}
\bibcite{newmanmodularity}{{100}{2006b}{{Newman}}{{}}}
\bibcite{clauset}{{101}{2016}{{Newman and Clauset}}{{}}}
\bibcite{newmangirvan}{{102}{2004}{{Newman and Girvan}}{{}}}
\bibcite{newmanspectral}{{103}{2006c}{{Newman}}{{}}}
\bibcite{NONCluster}{{104}{2015}{{Ni et~al.}}{{}}}
\bibcite{rWalk}{{105}{2004}{{Noh and Rieger}}{{}}}
\bibcite{taxonomy}{{106}{2012}{{Onnela et~al.}}{{}}}
\bibcite{parker2015network}{{107}{2015}{{Parker et~al.}}{{}}}
\bibcite{mlsbm1}{{108}{2015}{{Paul and Chen}}{{}}}
\bibcite{peelChange}{{109}{2015}{{Peel and Clauset}}{{}}}
\bibcite{peel2017ground}{{110}{2017}{{Peel et~al.}}{{}}}
\bibcite{peixotoHierarchAttribute}{{111}{2013}{{Peixoto}}{{}}}
\bibcite{tiagosbm}{{112}{2014}{{Peixoto}}{{}}}
\bibcite{thiagomlsbm}{{113}{2015}{{Peixoto}}{{}}}
\bibcite{peix}{{114}{2018}{{Peixoto}}{{}}}
\bibcite{peng}{{115}{2014}{{Peng et~al.}}{{}}}
\bibcite{flow}{{116}{2004}{{Perfetto et~al.}}{{}}}
\bibcite{perozziAttribute}{{117}{2018}{{Perozzi and Akoglu}}{{}}}
\bibcite{deepWalk}{{118}{2014}{{Perozzi et~al.}}{{}}}
\bibcite{porter2009communities}{{119}{2009a}{{Porter et~al.}}{{}}}
\bibcite{community}{{120}{2009b}{{Porter et~al.}}{{}}}
\bibcite{detectDegreeHetero}{{121}{2013}{{Radicchi}}{{}}}
\bibcite{resParam}{{122}{2006}{{Reichardt and Bornholdt}}{{}}}
\bibcite{detect21}{{123}{2008}{{Reichardt and Leone}}{{}}}
\bibcite{rombach2014core}{{124}{2014}{{Rombach et~al.}}{{}}}
\bibcite{VI}{{125}{2007}{{Rosenberg and Hirschberg}}{{}}}
\bibcite{HierarchAttl}{{126}{2013}{{Sarkar et~al.}}{{}}}
\bibcite{shaicase}{{127}{2017}{{Shai et~al.}}{{}}}
\bibcite{collabFilterReview}{{128}{2014}{{Shi et~al.}}{{}}}
\bibcite{gut}{{129}{2015}{{Shreiner et~al.}}{{}}}
\bibcite{originalSBM}{{130}{1997a}{{Snijders and Nowicki}}{{}}}
\bibcite{sbmorig}{{131}{1997b}{{Snijders and Nowicki}}{{}}}
\bibcite{linkComm}{{132}{2012}{{Soundarajan and Hopcroft}}{{}}}
\bibcite{compressing}{{133}{2017}{{Stanley et~al.}}{{}}}
\bibcite{smlsbm}{{134}{2016}{{Stanley et~al.}}{{}}}
\bibcite{taylor2015enhanced}{{135}{2015}{{Taylor et~al.}}{{}}}
\bibcite{gap}{{136}{2001}{{Tibshirani et~al.}}{{}}}
\bibcite{RWR}{{137}{2008}{{Tong et~al.}}{{}}}
\bibcite{traud}{{138}{2009}{{Traud et~al.}}{{}}}
\bibcite{Rand}{{139}{2011}{{Traud et~al.}}{{}}}
\bibcite{motiffinding}{{140}{2006}{{Tsuda and Kudo}}{{}}}
\bibcite{microbiome}{{141}{2007}{{Turnbaugh et~al.}}{{}}}
\bibcite{ugander2013subgraph}{{142}{2013}{{Ugander et~al.}}{{}}}
\bibcite{catala}{{143}{2016}{{Valles-Catala et~al.}}{{}}}
\bibcite{hub}{{144}{2013}{{van~den Heuvel and Sporns}}{{}}}
\bibcite{walsh2017}{{145}{2017}{{Walsh et~al.}}{{}}}
\bibcite{linkPredReview}{{146}{2014}{{Wang et~al.}}{{}}}
\bibcite{sbmdirect}{{147}{1987}{{Wang and Wong}}{{}}}
\bibcite{wong2015}{{148}{2015}{{Wong et~al.}}{{}}}
\bibcite{spectral1}{{149}{2008}{{Xiang and Gong}}{{}}}
\bibcite{LabProp}{{150}{2011}{{Xie and Szymanski}}{{}}}
\bibcite{affil}{{151}{2012}{{Yang and Leskovec}}{{}}}
\bibcite{bigclam}{{152}{2013}{{Yang and Leskovec}}{{}}}
\bibcite{jureTruth}{{153}{2015}{{Yang and Leskovec}}{{}}}
\bibcite{cesna}{{154}{2013}{{Yang et~al.}}{{}}}
\bibcite{SuperNodeSide}{{155}{2017}{{Yang et~al.}}{{}}}
\bibcite{your}{{156}{2014}{{Yourstone et~al.}}{{}}}
\bibcite{antagonism}{{157}{2015}{{Zapi{\'e}n-Campos et~al.}}{{}}}
\bibcite{FlowSpectral}{{158}{2010}{{Zare et~al.}}{{}}}
\bibcite{CoreHD}{{159}{2016}{{Zdeborov{\'a} et~al.}}{{}}}
\bibcite{comp}{{160}{2012}{{Zhang et~al.}}{{}}}
\bibcite{learning2}{{161}{2002}{{Zhu and Ghahramani}}{{}}}
\bibcite{ohmNet}{{162}{2017}{{Zitnik and Leskovec}}{{}}}
\bibcite{eNet}{{163}{2005}{{Zou and Hastie}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
